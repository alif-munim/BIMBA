{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a4cda5-3116-4548-80a1-25c454052768",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3a7a71eb-2ac3-4346-8261-150353bdee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3959803-8b07-4a93-a982-dd14c6616128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0 · imports & configuration\n",
    "# ============================================================\n",
    "import boto3, json, pandas as pd, itertools\n",
    "from tqdm.notebook import tqdm            # Jupyter/HTML bar\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "uris = [\n",
    "    \"s3://echodata25/results/echo-images/nova-pro/gen10-outputs/job_00/koh3yhoh38cy/prompts.jsonl.out\",\n",
    "    \"s3://echodata25/results/echo-images/nova-pro/gen10-outputs/job_01/zhnxl9kb6alc/prompts.jsonl.out\",\n",
    "    \"s3://echodata25/results/echo-images/nova-pro/gen10-outputs/job_02/h74k0w4wqyuy/prompts.jsonl.out\",\n",
    "    \"s3://echodata25/results/echo-images/nova-pro/gen10-outputs/job_03/0db4t3ctdamr/prompts.jsonl.out\",\n",
    "    \"s3://echodata25/results/echo-images/nova-pro/gen10-outputs/job_04/9xztwg01glwe/prompts.jsonl.out\",\n",
    "    \"s3://echodata25/results/echo-images/nova-pro/gen10-outputs/job_05/9ynxch4xlfeq/prompts.jsonl.out\",\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# helpers\n",
    "# ------------------------------------------------------------\n",
    "def body_iter_lines(uri: str):\n",
    "    bucket, key = uri.replace(\"s3://\", \"\", 1).split(\"/\", 1)\n",
    "    return boto3.client(\"s3\").get_object(Bucket=bucket, Key=key)[\"Body\"].iter_lines()\n",
    "\n",
    "def row_count(uri: str) -> int:\n",
    "    \"One streaming pass → exact line count.\"\n",
    "    return sum(1 for _ in body_iter_lines(uri))\n",
    "\n",
    "def stream_jsonl(uri: str):\n",
    "    for raw in body_iter_lines(uri):\n",
    "        if raw:\n",
    "            yield json.loads(raw)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# main loop\n",
    "# ------------------------------------------------------------\n",
    "frames = []\n",
    "outer = tqdm(uris, desc=\"all jobs\", unit=\"file\")   # overall progress\n",
    "\n",
    "for uri in outer:\n",
    "    job   = uri.split(\"/\")[-3]\n",
    "    total = row_count(uri)\n",
    "\n",
    "    rows = []\n",
    "    for rec in tqdm(stream_jsonl(uri),\n",
    "                    total=total,\n",
    "                    desc=job,\n",
    "                    unit=\"rows\",\n",
    "                    leave=True):      # keep each bar\n",
    "        rows.append(rec)\n",
    "\n",
    "    frames.append(pd.DataFrame.from_records(rows))\n",
    "    outer.update()                    # tick the master bar\n",
    "\n",
    "\n",
    "# final concatenation\n",
    "df_all = pd.concat(frames, ignore_index=True)\n",
    "print(\"Combined shape:\", df_all.shape)\n",
    "# df_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a01932ed-9223-47b9-9227-f83e7a4eaf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM total: 132.1 GB   free: 98.7 GB\n"
     ]
    }
   ],
   "source": [
    "# ---- Python (works in scripts / Jupyter) ----\n",
    "import os, psutil, multiprocessing\n",
    "\n",
    "ram = psutil.virtual_memory()\n",
    "print(f\"RAM total: {ram.total/1e9:.1f} GB   free: {ram.available/1e9:.1f} GB\")\n",
    "\n",
    "cpus_logical  = os.cpu_count()                 #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f0bbefc-d81b-4494-bd7c-3b92ff09b86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 logical cores\n"
     ]
    }
   ],
   "source": [
    "import os, multiprocessing as mp\n",
    "n_logical  = os.cpu_count()        # includes hyper-threads\n",
    "n_physical = mp.cpu_count()        # same on Linux; fallback\n",
    "\n",
    "print(n_logical, \"logical cores\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "68f8c722-42c8-4f6f-8579-60d6b101491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, orjson, re\n",
    "sid_pat = re.compile(r\"<SID:([^>]+)>\")\n",
    "\n",
    "def parse_row(row):\n",
    "    # fast JSON load only if still a string\n",
    "    mi = orjson.loads(row.modelInput)  if isinstance(row.modelInput,  str) else row.modelInput\n",
    "    mo = orjson.loads(row.modelOutput) if isinstance(row.modelOutput, str) else row.modelOutput\n",
    "\n",
    "    # ── SID ──────────────────────────────────────────────────────────────\n",
    "    sid = None\n",
    "    for m in mi.get(\"messages\", ()):\n",
    "        for seg in m.get(\"content\", ()):\n",
    "            if isinstance(seg, dict):\n",
    "                m0 = sid_pat.search(seg.get(\"text\", \"\"))\n",
    "                if m0:\n",
    "                    sid = m0.group(1)\n",
    "                    break\n",
    "        if sid: break\n",
    "\n",
    "    # ── conversation text ───────────────────────────────────────────────\n",
    "    try:\n",
    "        conv = mo[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    except Exception:\n",
    "        conv = None\n",
    "\n",
    "    return sid, conv\n",
    "\n",
    "# process rows lazily; no extra dataframe copies\n",
    "ids, convs = zip(*map(parse_row, df_all.itertuples(index=False)))\n",
    "\n",
    "out = pd.DataFrame({\"id\": ids, \"conversations\": convs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "59c1e98c-ce7e-414d-be98-f0dd2b0c0e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(266008, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "739dba56-f198-4fb4-a7c1-ee85d6c1042b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_master = pd.read_csv('hls_master_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4ebf48aa-f0f9-4a62-8212-fd19dc918e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out:  cols = [\"id\", \"conversations\"]\n",
    "# hls_master: cols include \"DeidentifiedStudyID\", \"study_dir\"\n",
    "\n",
    "# build a Series once → O(n) memory-light lookup table\n",
    "study_dir_map = (\n",
    "    hls_master\n",
    "    .set_index(\"DeidentifiedStudyID\")[\"study_dir\"]\n",
    "    .astype(str)          # make sure keys/vals are strings\n",
    ")\n",
    "\n",
    "# add column (vectorised; no join-copy)\n",
    "out[\"data_source\"] = out[\"id\"].map(study_dir_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "75af51fb-86e6-41f9-9cb3-b68b1095db37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc8ba057ce147a5abca29934fcd12af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266008 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prerequisites\n",
    "# pip install boto3 tqdm orjson\n",
    "\n",
    "import random, boto3, orjson, re, pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------- part 2 · pick one shuf*.mp4 per study --------------\n",
    "BUCKET   = \"echodata25\"\n",
    "ROOT     = \"results/echo-images/video-concat\"          # constant path prefix\n",
    "MATCHES  = (\"shuf1.mp4\", \"shuf2.mp4\")                  # wanted filenames\n",
    "MAX_WORKERS = 32                                       # tune for your net-bandwidth / vCPU\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "paginator = s3.get_paginator(\"list_objects_v2\")        # reused → keeps HTTP-pool alive\n",
    "\n",
    "def find_video(study_id: str, source_dir: str | float) -> str | None:\n",
    "    \"\"\"Return s3://…/shuf1|2.mp4 (random pick) or None.\"\"\"\n",
    "    if pd.isna(source_dir):\n",
    "        return None\n",
    "\n",
    "    prefix = f\"{ROOT}/{source_dir}/{study_id}/\"        # e.g. results/…/echo-study/<ID>/\n",
    "    try:\n",
    "        for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):\n",
    "            keys = [obj[\"Key\"] for obj in page.get(\"Contents\", ())\n",
    "                    if obj[\"Key\"].endswith(MATCHES)]\n",
    "            if keys:                                   # got one or more candidates\n",
    "                return f\"s3://{BUCKET}/{random.choice(keys)}\"\n",
    "    except s3.exceptions.NoSuchBucket:\n",
    "        pass                                           # bucket typo guard\n",
    "    return None\n",
    "\n",
    "# vectorised parallel lookup with progress bar\n",
    "ids   = out[\"id\"].values\n",
    "dirs  = out[\"data_source\"].values\n",
    "\n",
    "with ThreadPoolExecutor(MAX_WORKERS) as ex:\n",
    "    out[\"video\"] = list(\n",
    "        tqdm(ex.map(find_video, ids, dirs), total=len(out), unit=\"file\")\n",
    "    )\n",
    "\n",
    "# `out` now has columns: id · conversations · data_source · video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "74e88b1a-8c43-48fc-8fe4-ad7654f6ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d9a03f8d-cb21-4f6c-9cbd-ec3fad355a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: demjson3 in /opt/conda/lib/python3.12/site-packages (3.0.6)\n"
     ]
    }
   ],
   "source": [
    "! pip install demjson3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a8635469-985f-44de-8a0a-fe7ca3c9a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = out.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6e2c0723-2310-4a62-8fe3-b3ad21f2afbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68ab3537cf24e1d8dbf1db111e9df64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266008 [00:00<?, ?conv/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1213 rows truncated ➜ [366, 1501, 2620, 3930, 5297, 6029, 6868, 7208, 7825, 8270]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import re, json, orjson\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "df        = new.copy()                  # keep original safe\n",
    "COL       = \"conversations\"\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1 · structural typos\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "_fix_struct = (\n",
    "    (re.compile(r'\"value=\"\\s*'),                       '\"value\":\"'),\n",
    "    (re.compile(r'\\{\"from\":\"(gpt|human)\",\\s*\"\"'),      r'{\"from\":\"\\1\",\"value\":\"'),\n",
    "    (re.compile(r'\\{\"from\":\"(gpt|human)\"\\s*:\\s*'),     r'{\"from\":\"\\1\",\"value\":')\n",
    ")\n",
    "\n",
    "def structural(txt: str) -> str:\n",
    "    for pat, repl in _fix_struct:\n",
    "        txt = pat.sub(repl, txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2 · escape control chars & lone back-slashes inside strings\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "_str_pat = re.compile(r'\"(?:[^\"\\\\]|\\\\.)*\"', re.S)        # every JSON string\n",
    "\n",
    "def _escape_ctrl(ch: str) -> str:\n",
    "    \"\"\"Map control char to JSON escape sequence.\"\"\"\n",
    "    if   ch == '\\n': return r'\\n'\n",
    "    elif ch == '\\r': return r'\\r'\n",
    "    elif ch == '\\t': return r'\\t'\n",
    "    else:            return f'\\\\u{ord(ch):04x}'\n",
    "\n",
    "_ctrl_pat = re.compile(r'[\\x00-\\x1F]')                   # 0–31\n",
    "\n",
    "def escape_in_quotes(txt: str) -> str:\n",
    "    def patch(m):\n",
    "        s = m.group(0)\n",
    "        s = _ctrl_pat.sub(lambda c: _escape_ctrl(c.group(0)), s)\n",
    "        s = re.sub(r'\\\\(?![\"\\\\/bfnrtu])', r'\\\\\\\\', s)     # lone \"\\\"\n",
    "        return s\n",
    "    return _str_pat.sub(patch, txt)\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3 · convert raw → Python list   (None if truly truncated)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "def to_list(raw: str):\n",
    "    try:                         # fast path\n",
    "        return orjson.loads(raw)\n",
    "    except orjson.JSONDecodeError:\n",
    "        fixed = escape_in_quotes(structural(raw))\n",
    "        try:\n",
    "            return json.loads(fixed)  # stdlib tolerates pretty well\n",
    "        except Exception:\n",
    "            return None               # still broken\n",
    "\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4 · clean whole column with a progress bar\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "good, bad = [], []\n",
    "for i, txt in tqdm(enumerate(df[COL]), total=len(df), unit=\"conv\"):\n",
    "    parsed = to_list(txt)\n",
    "    if parsed is None:\n",
    "        bad.append(i)\n",
    "    else:\n",
    "        good.append(parsed)\n",
    "\n",
    "print(f\"{len(bad)} rows truncated ➜ {bad[:10]}\")\n",
    "\n",
    "# drop unrecoverable rows, assign cleaned lists\n",
    "df = df.drop(index=bad).reset_index(drop=True)\n",
    "df[COL] = good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3f7dd347-d984-482d-93e6-c9bef817669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dcdde132-16c0-40fb-a7e2-a9c3d7fbf1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = out.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b3cc3e76-5fbb-470a-9a50-59bbe3999623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # raw_df (or new) → the dataframe that still has the raw strings\n",
    "# # bad           → list/array of bad-row indices\n",
    "\n",
    "# def peek_rows(df, indices, n=20, ctx=120):\n",
    "#     \"\"\"\n",
    "#     Print a context slice around the JSON error byte for the first `n` indices.\n",
    "#     \"\"\"\n",
    "#     import orjson\n",
    "#     for idx in indices[:n]:\n",
    "#         txt = df.at[idx, \"conversations\"]\n",
    "#         try:\n",
    "#             orjson.loads(txt)            # will raise\n",
    "#         except orjson.JSONDecodeError as e:\n",
    "#             pos = e.pos                  # byte offset where parsing broke\n",
    "#             frag = txt[max(0, pos-ctx): pos+ctx]\n",
    "#             print(f\"\\n── row {idx}  (byte {pos}) ──\\n{frag}\\n\")\n",
    "\n",
    "# peek_rows(raw_df, bad, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f3c111cf-eaf9-45a7-bb64-277be1007b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.12/site-packages (from opencv-python-headless) (1.26.4)\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m174.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.11.0.86\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python-headless  # lighter, no GUI deps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "01f05906-ae5b-47da-a847-76535338a3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a29e609b53d4fb086dd9938663e47b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "videos:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled videos : 20\n",
      "avg frames     : 3506.2\n",
      "avg fps        : 30.00\n",
      "avg duration s : 116.87\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute average frame-count, FPS, and duration over the first `N`\n",
    "videos listed in df[\"video\"] (each value is an s3://…/shuf*.mp4 URI).\n",
    "\n",
    "Dependencies  (install once):\n",
    "    pip install boto3 opencv-python-headless tqdm\n",
    "\"\"\"\n",
    "\n",
    "import os, tempfile, boto3, cv2, pandas as pd, numpy as np\n",
    "from urllib.parse import urlparse\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "N        = 20                             # ← how many videos to sample\n",
    "tmp_dir  = tempfile.mkdtemp()\n",
    "s3       = boto3.client(\"s3\")\n",
    "\n",
    "frames, fpss, durs = [], [], []\n",
    "\n",
    "def download(uri: str, dest_dir: str) -> str:\n",
    "    \"\"\"Download S3 object to `dest_dir`, return local path.\"\"\"\n",
    "    parsed = urlparse(uri)\n",
    "    bucket, key = parsed.netloc, parsed.path.lstrip(\"/\")\n",
    "    local = os.path.join(dest_dir, os.path.basename(key))\n",
    "    if os.path.exists(local):                      # cached\n",
    "        return local\n",
    "    s3.download_file(bucket, key, local)\n",
    "    return local\n",
    "\n",
    "for uri in tqdm(df[\"video\"].iloc[:N], total=N, desc=\"videos\"):\n",
    "    path = download(uri, tmp_dir)\n",
    "\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    if not cap.isOpened():                         # skip broken files\n",
    "        continue\n",
    "    f = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    r = cap.get(cv2.CAP_PROP_FPS) or np.nan\n",
    "    cap.release()\n",
    "\n",
    "    frames.append(f)\n",
    "    fpss.append(r)\n",
    "    durs.append(f / r if r else np.nan)\n",
    "\n",
    "# ── report ──────────────────────────────────────────────────────────────\n",
    "print(f\"sampled videos : {len(frames)}\")\n",
    "print(f\"avg frames     : {np.nanmean(frames):.1f}\")\n",
    "print(f\"avg fps        : {np.nanmean(fpss):.2f}\")\n",
    "print(f\"avg duration s : {np.nanmean(durs):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bb0dd52b-0288-4018-9d69-509ee536fc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264795"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf99bb20-b6e3-4594-9fdc-8c827f484239",
   "metadata": {},
   "source": [
    "# Drop Rows Not Yet Processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64898b9-9dd1-4068-90c6-daa231a44a17",
   "metadata": {},
   "source": [
    "1. Also drop all of Syngo (54K rows) until it is reprocessed (findings column)\n",
    "2. Change all C63 to C61 (RA dilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a7102501-e95c-40e0-945a-79000787439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"video\"].notna() & (df[\"video\"] != \"None\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6b62fa05-87ee-4e85-9f92-87f2afa938cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130929"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "439a83f1-bd78-493a-b679-8bdd3951b8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     1.2.276.0.7230010.3.1.2.845494328.1.1703364833.18441940\n",
       "conversations    [{'from': 'human', 'value': '<image>\n",
       "Please provide an overview of cardiac structure and function you see here.'}, {'from': 'gpt', 'value': '<META_START>\n",
       "<C11> <C21> <C28> <C41> <C52> <C58> <C60> <C65> <C69> <C75>\n",
       "<META_END>\n",
       "\n",
       "The study shows a repaired Tetralogy of Fallot with a membranous ventricular septal defect and predominant left-to-right shunting. There is mild dilation of both atria and mild enlargement of the right ventricle. Global left ventricular function is normal, while right ventricular systolic function is mildly reduced.'}, {'from': 'human', 'value': 'Can you describe the appearance of the ventricular septal defect?'}, {'from': 'gpt', 'value': 'The ventricular septal defect appears as a small opening in the interventricular septum, demonstrated by color Doppler showing predominant left-to-right shunting with a peak gradient of 91 mmHg. The septal wall motion has a post-operative appearance, suggesting previous surgical intervention.'}, {'from': 'human', 'value': 'What are the characteristics of the tricuspid regurgitation observed?'}, {'from': 'gpt', 'value': 'The tricuspid regurgitation is moderate, with a visible jet extending into the right atrium. This is indicative of incomplete closure of the tricuspid valve during systole, leading to backflow of blood from the right ventricle to the right atrium.'}, {'from': 'human', 'value': 'Is there any evidence of aortic valve disease?'}, {'from': 'gpt', 'value': 'There is no evidence of aortic stenosis. However, mild aortic regurgitation is present, characterized by a small, localized jet seen on color Doppler, indicating slight backflow of blood from the aorta into the left ventricle during diastole.'}, {'from': 'human', 'value': 'What is the clinical significance of the mild pulmonic stenosis and severe pulmonic regurgitation?'}, {'from': 'gpt', 'value': 'The mild pulmonic stenosis may not significantly impact hemodynamics but should be monitored for progression. The severe pulmonic regurgitation, however, could lead to volume overload of the right ventricle, potentially contributing to the observed mild enlargement and reduced systolic function of the right ventricle.'}]\n",
       "data_source                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         echo-study\n",
       "video                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    s3://echodata25/results/echo-images/video-concat/echo-study/1.2.276.0.7230010.3.1.2.845494328.1.1703364833.18441940/1.2.276.0.7230010.3.1.3.845494328.1.1703364833.18441941/shuf2.mp4\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d85b355d-0b66-40ed-81ca-3c8d6e28c7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb38ef4612954052a9c2526796b4861e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "writing:   0%|          | 0/130929 [00:00<?, ?rows/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ wrote echomamba_130k.jsonl\n"
     ]
    }
   ],
   "source": [
    "df_ok = df\n",
    "OUT = \"BIMBA-LLaVA-NeXT/echomamba_130k.jsonl\"\n",
    "\n",
    "with open(OUT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in tqdm(df_ok.iterrows(), total=len(df_ok),\n",
    "                       desc=\"writing\", unit=\"rows\"):\n",
    "        rec = {\n",
    "            \"id\":          str(row[\"id\"]),\n",
    "            \"video\":       row[\"video\"],\n",
    "            \"conversations\": row[\"conversations\"],   # already a list of dicts\n",
    "        }\n",
    "\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"✅ wrote {OUT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0787be12-3e26-4cff-b230-0a1c115bb3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ uploaded to s3://echodata25/data/llava_video_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "local_file  = \"BIMBA-LLaVA-NeXT/echomamba_130k.jsonl\"\n",
    "bucket      = \"echodata25\"\n",
    "s3_key      = \"data/llava_video_dataset.jsonl\"\n",
    "\n",
    "s3.upload_file(local_file, bucket, s3_key)\n",
    "print(f\"✔️ uploaded to s3://{bucket}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5c88b87c-7fad-490a-9949-faf50ed4abb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1.2.276.0.7230010.3.1.2.1714512485.1.1703221347.12348238',\n",
       " 'video': 's3://echodata25/results/echo-images/video-concat/echo-study/1.2.276.0.7230010.3.1.2.1714512485.1.1703221347.12348238/1.2.276.0.7230010.3.1.3.1714512485.1.1703221347.12348239/shuf2.mp4',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': '<image>\\nGive me a concise first-pass assessment, including any reason for urgent intervention.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': '<META_START>\\n<C11> <C28> <C35> <C52> <C8>\\n<META_END>\\n\\nThe echocardiogram shows normal left ventricular function, normal valve structures, and no significant abnormalities. There is no urgent intervention required based on these findings.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Can you describe the appearance of the aortic valve in more detail?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The aortic valve is tricuspid with slightly thickened cusps, but there is no evidence of aortic stenosis. The valve appears to open and close normally without significant obstruction.'},\n",
       "  {'from': 'human', 'value': 'What about the left ventricular function?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The left ventricle shows normal systolic function with no major wall motion abnormalities. The chamber size is normal and the ventricular walls contract vigorously, indicating good contractility.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Are there any signs of regurgitation or stenosis in the other valves?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'There is trivial mitral regurgitation and trace tricuspid and pulmonic regurgitation present. The mitral, tricuspid, and pulmonic valves appear normal in structure and function without significant stenosis.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Is there any evidence of right ventricular dysfunction or pulmonary hypertension?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The right ventricular size and function are within normal limits. There is no evidence of pulmonary hypertension as the RVSP may be underestimated due to incomplete Doppler spectrum, but no severe gradients are noted.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'What are the clinical implications of these findings?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'The findings suggest normal cardiac structure and function with no immediate clinical concerns. Continued monitoring and routine follow-up are recommended.'}]}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf9a8a5-24a9-4cbc-83d3-30f24970c60b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "354f2542-561a-4fe7-84d6-b15eccc3eea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary: all good\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib, collections\n",
    "\n",
    "# path = \"BIMBA-LLaVA-NeXT/echomamba_130k.jsonl\"   # or the s3-downloaded copy\n",
    "path = \"BIMBA-LLaVA-NeXT/echomamba_130k_clean.jsonl\"\n",
    "problems = collections.Counter()\n",
    "\n",
    "with open(path) as f:\n",
    "    for n, line in enumerate(f, 1):\n",
    "        row = json.loads(line)\n",
    "        for t in row.get(\"conversations\", []):\n",
    "            if \"value\" not in t:                       # LLaVA expects this\n",
    "                problems[\"missing value\"] += 1\n",
    "                break\n",
    "            if not isinstance(t[\"value\"], str):\n",
    "                problems[\"not a string\"] += 1\n",
    "                break\n",
    "        else:\n",
    "            continue            # all turns in this sample are fine\n",
    "        print(\"❌  bad sample @ line\", n)              # first few is enough\n",
    "        if sum(problems.values()) > 20:\n",
    "            break\n",
    "\n",
    "print(\"summary:\", problems or \"all good\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "79680b12-2f51-4801-bd30-322577260da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ uploaded to s3://echodata25/data/llava_video_dataset_clean.jsonl\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "local_file  = \"BIMBA-LLaVA-NeXT/echomamba_130k_clean.jsonl\"\n",
    "bucket      = \"echodata25\"\n",
    "s3_key      = \"data/llava_video_dataset_clean.jsonl\"\n",
    "\n",
    "s3.upload_file(local_file, bucket, s3_key)\n",
    "print(f\"✔️ uploaded to s3://{bucket}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fc3df338-005b-4aa7-8cc0-c05630a229e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✂️  removed 1 bad rows → BIMBA-LLaVA-NeXT/echomamba_130k_clean.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json, pathlib\n",
    "\n",
    "src  = pathlib.Path(\"BIMBA-LLaVA-NeXT/echomamba_130k.jsonl\")\n",
    "dest = src.with_stem(src.stem + \"_clean\")           # echomamba_130k_clean.jsonl\n",
    "\n",
    "bad = 0\n",
    "with src.open() as fin, dest.open(\"w\") as fout:\n",
    "    for line in fin:\n",
    "        row = json.loads(line)\n",
    "        if any(\"value\" not in t for t in row.get(\"conversations\", [])):\n",
    "            bad += 1                # skip it\n",
    "            continue\n",
    "        fout.write(line)\n",
    "\n",
    "print(f\"✂️  removed {bad} bad rows → {dest}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7b58211c-91e7-426f-8cc2-c4eb6a3e691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# from sagemaker.pytorch import PyTorch\n",
    "# import sagemaker, boto3\n",
    "\n",
    "# role = sagemaker.get_execution_role()\n",
    "\n",
    "# # 👇  we’re already inside the BIMBA folder, so just use cwd()\n",
    "# SRC_DIR = Path.cwd()                         # /home/.../user-default-efs/BIMBA\n",
    "# REQ_FILE = SRC_DIR / \"BIMBA-LLaVA-NeXT\" / \"requirements.txt\"\n",
    "\n",
    "# estimator = PyTorch(\n",
    "#     entry_point      = \"train_entrypoint.py\",   # file is right here\n",
    "#     source_dir       = str(SRC_DIR),            # <-- fixed path\n",
    "#     dependencies     = [str(REQ_FILE)],\n",
    "#     role             = role,\n",
    "#     instance_type    = \"ml.p4d.24xlarge\",\n",
    "#     instance_count   = 1,\n",
    "#     framework_version = \"2.1\",\n",
    "#     py_version       = \"py310\",\n",
    "#     base_job_name    = \"bimba-train\",\n",
    "#     disable_profiler = True,\n",
    "# )\n",
    "\n",
    "# estimator.fit(\n",
    "#     inputs = {\n",
    "#         \"training\": \"s3://echodata25/data/llava_video_dataset_clean.jsonl\"\n",
    "#     },\n",
    "#     wait   = True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95cbbc-b391-4e1d-a92f-78846cb26074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/11/25 10:08:11] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> SageMaker Python SDK will collect telemetry to help us better  <a href=\"file:///opt/conda/lib/python3.12/site-packages/sagemaker/telemetry/telemetry_logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">telemetry_logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.12/site-packages/sagemaker/telemetry/telemetry_logging.py#91\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">91</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         understand our user's needs, diagnose issues, and deliver      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         additional features.                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To opt out of telemetry, please disable via TelemetryOptOut    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         parameter in SDK defaults config. For more information, refer  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         to                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://sagemaker.readthedocs.io/en/stable/overview.html#confi</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">guring-and-using-defaults-with-the-sagemaker-python-sdk.</span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/11/25 10:08:11]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m SageMaker Python SDK will collect telemetry to help us better  \u001b]8;id=549300;file:///opt/conda/lib/python3.12/site-packages/sagemaker/telemetry/telemetry_logging.py\u001b\\\u001b[2mtelemetry_logging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=938834;file:///opt/conda/lib/python3.12/site-packages/sagemaker/telemetry/telemetry_logging.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         understand our user's needs, diagnose issues, and deliver      \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         additional features.                                           \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To opt out of telemetry, please disable via TelemetryOptOut    \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         parameter in SDK defaults config. For more information, refer  \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         to                                                             \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://sagemaker.readthedocs.io/en/stable/overview.html#confi\u001b[0m \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mguring-and-using-defaults-with-the-sagemaker-python-sdk.\u001b[0m       \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/11/25 10:10:17] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating training-job with name: bimba-train-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-05-11-10-08-11-012   <a href=\"file:///opt/conda/lib/python3.12/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.12/site-packages/sagemaker/session.py#1042\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1042</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/11/25 10:10:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating training-job with name: bimba-train-\u001b[1;36m2025\u001b[0m-05-11-10-08-11-012   \u001b]8;id=741955;file:///opt/conda/lib/python3.12/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=516640;file:///opt/conda/lib/python3.12/site-packages/sagemaker/session.py#1042\u001b\\\u001b[2m1042\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-11 10:10:18 Starting - Starting the training job\n",
      "2025-05-11 10:10:18 Pending - Training job waiting for capacity......\n",
      "2025-05-11 10:11:10 Pending - Preparing the instances for training.....................\n",
      "2025-05-11 10:14:42 Downloading - Downloading input data...\n",
      "2025-05-11 10:14:57 Downloading - Downloading the training image.....................\n",
      "2025-05-11 10:18:55 Training - Training image download completed. Training in progress............\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34msed: can't read changehostname.c: No such file or directory\u001b[0m\n",
      "\u001b[34mgcc: error: changehostname.c: No such file or directory\u001b[0m\n",
      "\u001b[34mgcc: fatal error: no input files\u001b[0m\n",
      "\u001b[34mcompilation terminated.\u001b[0m\n",
      "\u001b[34mgcc: error: changehostname.o: No such file or directory\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2025-05-11 10:20:32,312 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2025-05-11 10:20:32,406 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-05-11 10:20:32,415 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-05-11 10:20:32,416 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-05-11 10:20:44,071 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mCollecting transformers@ git+https://github.com/huggingface/transformers.git@1c39974a4c4036fd641bc1191cc32799f85715a4 (from -r requirements.txt (line 163))\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/transformers.git (to revision 1c39974a4c4036fd641bc1191cc32799f85715a4) to /tmp/pip-install-1xcu0tyn/transformers_96ad0cadb880477dafad91ed378960d4\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-1xcu0tyn/transformers_96ad0cadb880477dafad91ed378960d4\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mRunning command git rev-parse -q --verify 'sha^1c39974a4c4036fd641bc1191cc32799f85715a4'\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mRunning command git fetch -q https://github.com/huggingface/transformers.git 1c39974a4c4036fd641bc1191cc32799f85715a4\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mRunning command git checkout -q 1c39974a4c4036fd641bc1191cc32799f85715a4\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/transformers.git to commit 1c39974a4c4036fd641bc1191cc32799f85715a4\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.29.3 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.29.3-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohappyeyeballs==2.4.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp==3.11.11 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (3.11.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal==1.3.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: anyio==4.8.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout==5.0.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (5.0.1)\u001b[0m\n",
      "\u001b[34mCollecting attrs==24.3.0 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: av==14.0.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (14.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: beautifulsoup4==4.13.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (4.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: bitsandbytes==0.41.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (0.41.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: black==24.1.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (24.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: bleach==6.2.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (6.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: causal-conv1d==1.4.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi==2024.12.14 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (2024.12.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cfgv==3.4.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (3.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet==5.2.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (5.2.0)\u001b[0m\n",
      "\u001b[34mCollecting charset-normalizer==3.4.1 (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mDownloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\u001b[0m\n",
      "\u001b[34mCollecting click==8.1.8 (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mDownloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: colorama==0.4.6 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (0.4.6)\u001b[0m\n",
      "\u001b[34mCollecting contourpy==1.3.1 (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler==0.12.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: DataProperty==1.1.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets==2.16.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (2.16.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decord==0.6.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: deepspeed==0.14.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (0.14.4)\u001b[0m\n",
      "\u001b[34mCollecting dill==0.3.7 (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mDownloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: distlib==0.3.9 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 27)) (0.3.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: distro==1.9.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 28)) (1.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docker-pycreds==0.4.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 29)) (0.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docstring_parser==0.16 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 30)) (0.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops==0.6.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 31)) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops-exts==0.0.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 32)) (0.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: et_xmlfile==2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 33)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: evaluate==0.4.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 34)) (0.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: exceptiongroup==1.2.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 35)) (1.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fastapi==0.115.7 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 36)) (0.115.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ffmpeg==1.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 37)) (1.4)\u001b[0m\n",
      "\u001b[34mCollecting filelock==3.17.0 (from -r requirements.txt (line 38))\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting fonttools==4.55.6 (from -r requirements.txt (line 39))\u001b[0m\n",
      "\u001b[34mDownloading fonttools-4.55.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (166 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist==1.5.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 40)) (1.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec==2023.10.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 41)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ftfy==6.3.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 42)) (6.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gdown==5.2.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 43)) (5.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gitdb==4.0.12 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 44)) (4.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: GitPython==3.1.44 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 45)) (3.1.44)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gradio_client==0.2.9 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 46)) (0.2.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: h11==0.14.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 47)) (0.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hf_transfer==0.1.9 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 48)) (0.1.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson==3.1.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 49)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: httpcore==0.16.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 50)) (0.16.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: httpx==0.24.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 51)) (0.24.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub==0.27.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 52)) (0.27.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: identify==2.6.6 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 53)) (2.6.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna==3.10 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 54)) (3.10)\u001b[0m\n",
      "\u001b[34mCollecting imageio==2.37.0 (from -r requirements.txt (line 55))\u001b[0m\n",
      "\u001b[34mDownloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: isort==5.13.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 56)) (5.13.2)\u001b[0m\n",
      "\u001b[34mCollecting Jinja2==3.1.5 (from -r requirements.txt (line 57))\u001b[0m\n",
      "\u001b[34mDownloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jiter==0.8.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 58)) (0.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib==1.4.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 59)) (1.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jsonlines==4.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 60)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kaggle==1.6.17 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 61)) (1.6.17)\u001b[0m\n",
      "\u001b[34mCollecting kiwisolver==1.4.8 (from -r requirements.txt (line 62))\u001b[0m\n",
      "\u001b[34mDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: latex2mathml==3.77.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 63)) (3.77.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: loguru==0.7.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 64)) (0.7.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: lxml==5.3.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 65)) (5.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mamba-ssm==2.2.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 66)) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py==3.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 67)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown2==2.5.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 68)) (2.5.2)\u001b[0m\n",
      "\u001b[34mCollecting MarkupSafe==3.0.2 (from -r requirements.txt (line 69))\u001b[0m\n",
      "\u001b[34mDownloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib==3.10.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 70)) (3.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mbstrdecoder==1.1.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 71)) (1.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl==0.1.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 72)) (0.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath==1.3.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 73)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict==6.1.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 74)) (6.1.0)\u001b[0m\n",
      "\u001b[34mCollecting multiprocess==0.70.15 (from -r requirements.txt (line 75))\u001b[0m\n",
      "\u001b[34mDownloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mypy-extensions==1.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 76)) (1.0.0)\u001b[0m\n",
      "\u001b[34mCollecting networkx==3.4.2 (from -r requirements.txt (line 77))\u001b[0m\n",
      "\u001b[34mDownloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting ninja==1.11.1.3 (from -r requirements.txt (line 78))\u001b[0m\n",
      "\u001b[34mDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nltk==3.9.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 79)) (3.9.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nodeenv==1.9.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 80)) (1.9.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numexpr==2.10.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 81)) (2.10.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy==1.26.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 82)) (1.26.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 83)) (12.1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 84)) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 85)) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 86)) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 87)) (8.9.2.26)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 88)) (11.0.2.54)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 89)) (10.3.2.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 90)) (11.4.5.107)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 91)) (12.1.0.106)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-ml-py==12.560.30 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 92)) (12.560.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 93)) (2.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 94)) (12.6.85)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 95)) (12.1.105)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: open_clip_torch==2.30.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 96)) (2.30.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: openai==1.60.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 97)) (1.60.1)\u001b[0m\n",
      "\u001b[34mCollecting opencv-python==4.11.0.86 (from -r requirements.txt (line 98))\u001b[0m\n",
      "\u001b[34mDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: opencv-python-headless==4.11.0.86 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 99)) (4.11.0.86)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: openpyxl==3.1.5 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 100)) (3.1.5)\u001b[0m\n",
      "\u001b[34mCollecting packaging==24.2 (from -r requirements.txt (line 101))\u001b[0m\n",
      "\u001b[34mDownloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting pandas==2.2.3 (from -r requirements.txt (line 102))\u001b[0m\n",
      "\u001b[34mDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathspec==0.12.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 103)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathvalidate==3.2.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 104)) (3.2.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: peft==0.4.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 105)) (0.4.0)\u001b[0m\n",
      "\u001b[34mCollecting pillow==11.1.0 (from -r requirements.txt (line 106))\u001b[0m\n",
      "\u001b[34mDownloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting platformdirs==4.3.6 (from -r requirements.txt (line 107))\u001b[0m\n",
      "\u001b[34mDownloading platformdirs-4.3.6-py3-none-any.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: portalocker==3.1.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 108)) (3.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pre_commit==4.1.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 109)) (4.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: propcache==0.2.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 110)) (0.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf==3.20.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 111)) (3.20.0)\u001b[0m\n",
      "\u001b[34mCollecting psutil==6.1.1 (from -r requirements.txt (line 112))\u001b[0m\n",
      "\u001b[34mDownloading psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo==9.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 113)) (9.0.0)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow==19.0.0 (from -r requirements.txt (line 114))\u001b[0m\n",
      "\u001b[34mDownloading pyarrow-19.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix==0.6 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 115)) (0.6)\u001b[0m\n",
      "\u001b[34mCollecting pybind11==2.13.6 (from -r requirements.txt (line 116))\u001b[0m\n",
      "\u001b[34mDownloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycocoevalcap==1.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 117)) (1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycocotools==2.0.8 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 118)) (2.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic==1.10.8 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 119)) (1.10.8)\u001b[0m\n",
      "\u001b[34mCollecting Pygments==2.19.1 (from -r requirements.txt (line 120))\u001b[0m\n",
      "\u001b[34mDownloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyparsing==3.2.1 (from -r requirements.txt (line 121))\u001b[0m\n",
      "\u001b[34mDownloading pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PySocks==1.7.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 122)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytablewriter==1.2.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 123)) (1.2.1)\u001b[0m\n",
      "\u001b[34mCollecting python-dateutil==2.9.0.post0 (from -r requirements.txt (line 124))\u001b[0m\n",
      "\u001b[34mDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-slugify==8.0.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 125)) (8.0.4)\u001b[0m\n",
      "\u001b[34mCollecting pytz==2024.2 (from -r requirements.txt (line 126))\u001b[0m\n",
      "\u001b[34mDownloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML==6.0.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 127)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex==2024.11.6 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 128)) (2024.11.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests==2.32.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 129)) (2.32.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rfc3986==1.5.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 130)) (1.5.0)\u001b[0m\n",
      "\u001b[34mCollecting rich==13.9.4 (from -r requirements.txt (line 131))\u001b[0m\n",
      "\u001b[34mDownloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sacrebleu==2.5.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 132)) (2.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors==0.5.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 133)) (0.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn==1.2.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 134)) (1.2.2)\u001b[0m\n",
      "\u001b[34mCollecting scipy==1.15.1 (from -r requirements.txt (line 135))\u001b[0m\n",
      "\u001b[34mDownloading scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece==0.1.99 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 136)) (0.1.99)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentry-sdk==2.20.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 137)) (2.20.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setproctitle==1.3.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 138)) (1.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shortuuid==1.0.13 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 139)) (1.0.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shtab==1.7.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 140)) (1.7.1)\u001b[0m\n",
      "\u001b[34mCollecting six==1.17.0 (from -r requirements.txt (line 141))\u001b[0m\n",
      "\u001b[34mDownloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smmap==5.0.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 142)) (5.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sniffio==1.3.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 143)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: soupsieve==2.6 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 144)) (2.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sqlitedict==2.1.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 145)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: starlette==0.45.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 146)) (0.45.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: svgwrite==1.4.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 147)) (1.4.3)\u001b[0m\n",
      "\u001b[34mCollecting sympy==1.13.3 (from -r requirements.txt (line 148))\u001b[0m\n",
      "\u001b[34mDownloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tabledata==1.3.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 149)) (1.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tabulate==0.9.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 150)) (0.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tcolorpy==0.1.7 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 151)) (0.1.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tenacity==8.3.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 152)) (8.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: text-unidecode==1.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 153)) (1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl==3.5.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 154)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tiktoken==0.8.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 155)) (0.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: timm==1.0.14 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 156)) (1.0.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers==0.15.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 157)) (0.15.2)\u001b[0m\n",
      "\u001b[34mCollecting tomli==2.2.1 (from -r requirements.txt (line 158))\u001b[0m\n",
      "\u001b[34mDownloading tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting torch==2.1.2 (from -r requirements.txt (line 159))\u001b[0m\n",
      "\u001b[34mDownloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting torchvision==0.16.2 (from -r requirements.txt (line 160))\u001b[0m\n",
      "\u001b[34mDownloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm==4.67.1 (from -r requirements.txt (line 161))\u001b[0m\n",
      "\u001b[34mDownloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm-multiprocess==0.0.11 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 162)) (0.0.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: transformers-stream-generator==0.0.5 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 164)) (0.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 165)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typeguard==4.4.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 166)) (4.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typepy==1.3.4 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 167)) (1.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing_extensions==4.12.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 168)) (4.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tyro==0.9.12 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 169)) (0.9.12)\u001b[0m\n",
      "\u001b[34mCollecting tzdata==2025.1 (from -r requirements.txt (line 170))\u001b[0m\n",
      "\u001b[34mDownloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting urllib3==1.26.20 (from -r requirements.txt (line 171))\u001b[0m\n",
      "\u001b[34mDownloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: uvicorn==0.34.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 172)) (0.34.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: virtualenv==20.29.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 173)) (20.29.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wandb==0.18.7 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 174)) (0.18.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wavedrom==2.0.3.post3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 175)) (2.0.3.post3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth==0.2.13 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 176)) (0.2.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: webencodings==0.5.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 177)) (0.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: websockets==14.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 178)) (14.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash==3.5.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 179)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl==1.18.3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 180)) (1.18.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yt-dlp==2025.1.15 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 181)) (2025.1.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zss==1.2.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 182)) (1.2.0)\u001b[0m\n",
      "\u001b[34mCollecting zstandard==0.23.0 (from -r requirements.txt (line 183))\u001b[0m\n",
      "\u001b[34mDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=38.3.0 in /opt/conda/lib/python3.10/site-packages (from pytablewriter==1.2.1->-r requirements.txt (line 123)) (72.1.0)\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.29.3-py3-none-any.whl (297 kB)\u001b[0m\n",
      "\u001b[34mDownloading attrs-24.3.0-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34mDownloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\u001b[0m\n",
      "\u001b[34mDownloading click-8.1.8-py3-none-any.whl (98 kB)\u001b[0m\n",
      "\u001b[34mDownloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\u001b[0m\n",
      "\u001b[34mDownloading dill-0.3.7-py3-none-any.whl (115 kB)\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.17.0-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mDownloading fonttools-4.55.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/4.6 MB 117.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading imageio-2.37.0-py3-none-any.whl (315 kB)\u001b[0m\n",
      "\u001b[34mDownloading jinja2-3.1.5-py3-none-any.whl (134 kB)\u001b[0m\n",
      "\u001b[34mDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 161.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\u001b[0m\n",
      "\u001b[34mDownloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\u001b[0m\n",
      "\u001b[34mDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 155.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\u001b[0m\n",
      "\u001b[34mDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.0/63.0 MB 168.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading packaging-24.2-py3-none-any.whl (65 kB)\u001b[0m\n",
      "\u001b[34mDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 168.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 173.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading platformdirs-4.3.6-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mDownloading psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\u001b[0m\n",
      "\u001b[34mDownloading pyarrow-19.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.1/42.1 MB 165.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\u001b[0m\n",
      "\u001b[34mDownloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 132.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\u001b[0m\n",
      "\u001b[34mDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\u001b[0m\n",
      "\u001b[34mDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\u001b[0m\n",
      "\u001b[34mDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\u001b[0m\n",
      "\u001b[34mDownloading rich-13.9.4-py3-none-any.whl (242 kB)\u001b[0m\n",
      "\u001b[34mDownloading scipy-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.6/40.6 MB 180.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\u001b[0m\n",
      "\u001b[34mDownloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 163.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tomli-2.2.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mDownloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 670.2/670.2 MB 26.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/6.8 MB 31.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34mDownloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\u001b[0m\n",
      "\u001b[34mDownloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 30.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, zstandard, urllib3, tzdata, tqdm, tomli, sympy, six, scipy, pyparsing, Pygments, pybind11, pyarrow, psutil, platformdirs, pillow, packaging, opencv-python, ninja, networkx, MarkupSafe, kiwisolver, fonttools, filelock, dill, contourpy, click, charset-normalizer, attrs, rich, python-dateutil, multiprocess, Jinja2, imageio, torch, pandas, torchvision, accelerate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: pytz\u001b[0m\n",
      "\u001b[34mFound existing installation: pytz 2024.1\u001b[0m\n",
      "\u001b[34mUninstalling pytz-2024.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled pytz-2024.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: zstandard\u001b[0m\n",
      "\u001b[34mFound existing installation: zstandard 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling zstandard-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled zstandard-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: urllib3\u001b[0m\n",
      "\u001b[34mFound existing installation: urllib3 1.26.19\u001b[0m\n",
      "\u001b[34mUninstalling urllib3-1.26.19:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled urllib3-1.26.19\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tzdata\u001b[0m\n",
      "\u001b[34mFound existing installation: tzdata 2024.1\u001b[0m\n",
      "\u001b[34mUninstalling tzdata-2024.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tzdata-2024.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tqdm\u001b[0m\n",
      "\u001b[34mFound existing installation: tqdm 4.66.5\u001b[0m\n",
      "\u001b[34mUninstalling tqdm-4.66.5:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tqdm-4.66.5\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tomli\u001b[0m\n",
      "\u001b[34mFound existing installation: tomli 2.0.1\u001b[0m\n",
      "\u001b[34mUninstalling tomli-2.0.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tomli-2.0.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: sympy\u001b[0m\n",
      "\u001b[34mFound existing installation: sympy 1.13.0\u001b[0m\n",
      "\u001b[34mUninstalling sympy-1.13.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled sympy-1.13.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: six\u001b[0m\n",
      "\u001b[34mFound existing installation: six 1.16.0\u001b[0m\n",
      "\u001b[34mUninstalling six-1.16.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled six-1.16.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scipy\u001b[0m\n",
      "\u001b[34mFound existing installation: scipy 1.14.0\u001b[0m\n",
      "\u001b[34mUninstalling scipy-1.14.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scipy-1.14.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: pyparsing\u001b[0m\n",
      "\u001b[34mFound existing installation: pyparsing 3.1.2\u001b[0m\n",
      "\u001b[34mUninstalling pyparsing-3.1.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled pyparsing-3.1.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: Pygments\u001b[0m\n",
      "\u001b[34mFound existing installation: Pygments 2.18.0\u001b[0m\n",
      "\u001b[34mUninstalling Pygments-2.18.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled Pygments-2.18.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: pybind11\u001b[0m\n",
      "\u001b[34mFound existing installation: pybind11 2.13.1\u001b[0m\n",
      "\u001b[34mUninstalling pybind11-2.13.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled pybind11-2.13.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: pyarrow\u001b[0m\n",
      "\u001b[34mFound existing installation: pyarrow 17.0.0\u001b[0m\n",
      "\u001b[34mUninstalling pyarrow-17.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled pyarrow-17.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: psutil\u001b[0m\n",
      "\u001b[34mFound existing installation: psutil 6.0.0\u001b[0m\n",
      "\u001b[34mUninstalling psutil-6.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled psutil-6.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: platformdirs\u001b[0m\n",
      "\u001b[34mFound existing installation: platformdirs 4.2.2\u001b[0m\n",
      "\u001b[34mUninstalling platformdirs-4.2.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled platformdirs-4.2.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: pillow\u001b[0m\n",
      "\u001b[34mFound existing installation: pillow 10.3.0\u001b[0m\n",
      "\u001b[34mUninstalling pillow-10.3.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled pillow-10.3.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: packaging\u001b[0m\n",
      "\u001b[34mFound existing installation: packaging 23.1\u001b[0m\n",
      "\u001b[34mUninstalling packaging-23.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled packaging-23.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: opencv-python\u001b[0m\n",
      "\u001b[34mFound existing installation: opencv-python 4.10.0.84\u001b[0m\n",
      "\u001b[34mUninstalling opencv-python-4.10.0.84:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled opencv-python-4.10.0.84\u001b[0m\n",
      "\u001b[34mAttempting uninstall: ninja\u001b[0m\n",
      "\u001b[34mFound existing installation: ninja 1.11.1.1\u001b[0m\n",
      "\u001b[34mUninstalling ninja-1.11.1.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled ninja-1.11.1.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: networkx\u001b[0m\n",
      "\u001b[34mFound existing installation: networkx 3.3\u001b[0m\n",
      "\u001b[34mUninstalling networkx-3.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled networkx-3.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: MarkupSafe\u001b[0m\n",
      "\u001b[34mFound existing installation: MarkupSafe 2.1.5\u001b[0m\n",
      "\u001b[34mUninstalling MarkupSafe-2.1.5:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled MarkupSafe-2.1.5\u001b[0m\n",
      "\u001b[34mAttempting uninstall: kiwisolver\u001b[0m\n",
      "\u001b[34mFound existing installation: kiwisolver 1.4.5\u001b[0m\n",
      "\u001b[34mUninstalling kiwisolver-1.4.5:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled kiwisolver-1.4.5\u001b[0m\n",
      "\u001b[34mAttempting uninstall: fonttools\u001b[0m\n",
      "\u001b[34mFound existing installation: fonttools 4.53.1\u001b[0m\n",
      "\u001b[34mUninstalling fonttools-4.53.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled fonttools-4.53.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: filelock\u001b[0m\n",
      "\u001b[34mFound existing installation: filelock 3.15.4\u001b[0m\n",
      "\u001b[34mUninstalling filelock-3.15.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled filelock-3.15.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: dill\u001b[0m\n",
      "\u001b[34mFound existing installation: dill 0.3.8\u001b[0m\n",
      "\u001b[34mUninstalling dill-0.3.8:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled dill-0.3.8\u001b[0m\n",
      "\u001b[34mAttempting uninstall: contourpy\u001b[0m\n",
      "\u001b[34mFound existing installation: contourpy 1.2.1\u001b[0m\n",
      "\u001b[34mUninstalling contourpy-1.2.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled contourpy-1.2.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: click\u001b[0m\n",
      "\u001b[34mFound existing installation: click 8.1.7\u001b[0m\n",
      "\u001b[34mUninstalling click-8.1.7:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled click-8.1.7\u001b[0m\n",
      "\u001b[34mAttempting uninstall: charset-normalizer\u001b[0m\n",
      "\u001b[34mFound existing installation: charset-normalizer 3.2.0\u001b[0m\n",
      "\u001b[34mUninstalling charset-normalizer-3.2.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled charset-normalizer-3.2.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: attrs\u001b[0m\n",
      "\u001b[34mFound existing installation: attrs 23.2.0\u001b[0m\n",
      "\u001b[34mUninstalling attrs-23.2.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled attrs-23.2.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: rich\u001b[0m\n",
      "\u001b[34mFound existing installation: rich 13.7.1\u001b[0m\n",
      "\u001b[34mUninstalling rich-13.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled rich-13.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: python-dateutil\u001b[0m\n",
      "\u001b[34mFound existing installation: python-dateutil 2.9.0\u001b[0m\n",
      "\u001b[34mUninstalling python-dateutil-2.9.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled python-dateutil-2.9.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: multiprocess\u001b[0m\n",
      "\u001b[34mFound existing installation: multiprocess 0.70.16\u001b[0m\n",
      "\u001b[34mUninstalling multiprocess-0.70.16:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled multiprocess-0.70.16\u001b[0m\n",
      "\u001b[34mAttempting uninstall: Jinja2\u001b[0m\n",
      "\u001b[34mFound existing installation: Jinja2 3.1.4\u001b[0m\n",
      "\u001b[34mUninstalling Jinja2-3.1.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled Jinja2-3.1.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: imageio\u001b[0m\n",
      "\u001b[34mFound existing installation: imageio 2.34.2\u001b[0m\n",
      "\u001b[34mUninstalling imageio-2.34.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled imageio-2.34.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.1.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.1.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.1.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: pandas\u001b[0m\n",
      "\u001b[34mFound existing installation: pandas 2.2.2\u001b[0m\n",
      "\u001b[34mUninstalling pandas-2.2.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled pandas-2.2.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torchvision\u001b[0m\n",
      "\u001b[34mFound existing installation: torchvision 0.16.0\u001b[0m\n",
      "\u001b[34mUninstalling torchvision-0.16.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torchvision-0.16.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.22.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.22.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.22.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mpathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\u001b[0m\n",
      "\u001b[34mpathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker 2.228.0 requires attrs<24,>=23.1.0, but you have attrs 24.3.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Jinja2-3.1.5 MarkupSafe-3.0.2 Pygments-2.19.1 accelerate-0.29.3 attrs-24.3.0 charset-normalizer-3.4.1 click-8.1.8 contourpy-1.3.1 dill-0.3.7 filelock-3.17.0 fonttools-4.55.6 imageio-2.37.0 kiwisolver-1.4.8 multiprocess-0.70.15 networkx-3.4.2 ninja-1.11.1.3 opencv-python-4.11.0.86 packaging-24.2 pandas-2.2.3 pillow-11.1.0 platformdirs-4.3.6 psutil-6.1.1 pyarrow-19.0.0 pybind11-2.13.6 pyparsing-3.2.1 python-dateutil-2.9.0.post0 pytz-2024.2 rich-13.9.4 scipy-1.15.1 six-1.17.0 sympy-1.13.3 tomli-2.2.1 torch-2.1.2 torchvision-0.16.2 tqdm-4.67.1 tzdata-2025.1 urllib3-1.26.20 zstandard-0.23.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.1.2 -> 25.1.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2025-05-11 10:22:01,990 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-05-11 10:22:01,990 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2025-05-11 10:22:02,113 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2025-05-11 10:22:02,216 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m2025-05-11 10:22:02,315 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-05-11 10:22:02,325 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"bimba-train-2025-05-11-10-08-11-012\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-495467399120/bimba-train-2025-05-11-10-08-11-012/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_entrypoint\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_entrypoint.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_entrypoint.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_entrypoint\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-495467399120/bimba-train-2025-05-11-10-08-11-012/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"bimba-train-2025-05-11-10-08-11-012\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-495467399120/bimba-train-2025-05-11-10-08-11-012/source/sourcedir.tar.gz\",\"module_name\":\"train_entrypoint\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_entrypoint.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages:__editable__.llava-1.7.0.dev0.finder.__path_hook__\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train_entrypoint.py\u001b[0m\n",
      "\u001b[34m2025-05-11 10:22:02,326 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2025-05-11 10:22:02,326 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m[INFO] Training JSONL: /opt/ml/input/data/training/llava_video_dataset_clean.jsonl\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.1.2 -> 25.1.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.1.2 -> 25.1.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mERROR: Invalid wheel filename (wrong number of parts): 'flash_attn-2.7.4.post1+cu121torch2.1cxx11abiFALSE-cp310-cp310_linux_x86_64'\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mDEPRECATION: Building 'flash-attn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'flash-attn'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mtransformer-engine 0.12.0+170797 requires flash-attn<=2.0.4,>=1.0.6, but you have flash-attn 2.4.3.post1 which is incompatible.\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.1.2 -> 25.1.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[INFO] Patched exp.yaml -> /opt/ml/input/data/training/llava_video_dataset_clean.jsonl\u001b[0m\n",
      "\u001b[34m[INFO] Starting BIMBA training ...\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:25,845] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mdf: /root/.triton/autotune\u001b[0m\n",
      "\u001b[34m: No such file or directory\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:29,849] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:29,849] [INFO] [runner.py:568:main] cmd = /opt/conda/bin/python3.10 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=30000 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed scripts/zero3.json --model_name_or_path lmms-lab/LLaVA-Video-7B-Qwen2 --lora_enable True --lora_r 128 --lora_alpha 32 --version qwen_1_5 --data_path /opt/ml/code/BIMBA-LLaVA-NeXT/scripts/video/train/exp.yaml --image_folder /tmp/empty_images --video_folder /tmp --mm_tunable_parts=mm_vision_tower,mm_mlp_adapter,mm_language_model --mm_vision_tower_lr=2e-6 --vision_tower google/siglip-so400m-patch14-384 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --compressor_type bimba --group_by_modality_length True --image_aspect_ratio anyres_max_9 --image_grid_pinpoints (1x1),...,(6x6) --mm_patch_merge_type spatial_unpad --bf16 True --run_name BIMBA_LLaVA_Qwen2_7B --output_dir work_dirs/BIMBA_LLaVA_Qwen2_7B --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 4 --gradient_accumulation_steps 2 --evaluation_strategy no --save_strategy steps --save_steps 500 --save_total_limit 100 --learning_rate 1e-6 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 8192 --gradient_checkpointing True --dataloader_num_workers 2 --lazy_preprocess True --torch_compile True --torch_compile_backend inductor --dataloader_drop_last True --frames_upbound 16 --mm_newline_position grid --add_time_instruction True --force_sample True --mm_spatial_pool_stride 2 --report_to none\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:32,149] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,101] [INFO] [launch.py:139:main] 0 NCCL_VERSION=2.18.5\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,101] [INFO] [launch.py:139:main] 0 NCCL_SOCKET_IFNAME=eth0\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,101] [INFO] [launch.py:139:main] 0 NCCL_DEBUG=WARN\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,101] [INFO] [launch.py:139:main] 0 NCCL_ASYNC_ERROR_HANDLING=1\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,101] [INFO] [launch.py:139:main] 0 NCCL_IB_DISABLE=1\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,101] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,101] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,101] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,101] [INFO] [launch.py:164:main] dist_world_size=8\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,101] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,102] [INFO] [launch.py:256:main] process 692 spawned with command: ['/opt/conda/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--deepspeed', 'scripts/zero3.json', '--model_name_or_path', 'lmms-lab/LLaVA-Video-7B-Qwen2', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '32', '--version', 'qwen_1_5', '--data_path', '/opt/ml/code/BIMBA-LLaVA-NeXT/scripts/video/train/exp.yaml', '--image_folder', '/tmp/empty_images', '--video_folder', '/tmp', '--mm_tunable_parts=mm_vision_tower,mm_mlp_adapter,mm_language_model', '--mm_vision_tower_lr=2e-6', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--compressor_type', 'bimba', '--group_by_modality_length', 'True', '--image_aspect_ratio', 'anyres_max_9', '--image_grid_pinpoints', '(1x1),...,(6x6)', '--mm_patch_merge_type', 'spatial_unpad', '--bf16', 'True', '--run_name', 'BIMBA_LLaVA_Qwen2_7B', '--output_dir', 'work_dirs/BIMBA_LLaVA_Qwen2_7B', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '100', '--learning_rate', '1e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '8192', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '2', '--lazy_preprocess', 'True', '--torch_compile', 'True', '--torch_compile_backend', 'inductor', '--dataloader_drop_last', 'True', '--frames_upbound', '16', '--mm_newline_position', 'grid', '--add_time_instruction', 'True', '--force_sample', 'True', '--mm_spatial_pool_stride', '2', '--report_to', 'none']\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,103] [INFO] [launch.py:256:main] process 693 spawned with command: ['/opt/conda/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=1', '--deepspeed', 'scripts/zero3.json', '--model_name_or_path', 'lmms-lab/LLaVA-Video-7B-Qwen2', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '32', '--version', 'qwen_1_5', '--data_path', '/opt/ml/code/BIMBA-LLaVA-NeXT/scripts/video/train/exp.yaml', '--image_folder', '/tmp/empty_images', '--video_folder', '/tmp', '--mm_tunable_parts=mm_vision_tower,mm_mlp_adapter,mm_language_model', '--mm_vision_tower_lr=2e-6', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--compressor_type', 'bimba', '--group_by_modality_length', 'True', '--image_aspect_ratio', 'anyres_max_9', '--image_grid_pinpoints', '(1x1),...,(6x6)', '--mm_patch_merge_type', 'spatial_unpad', '--bf16', 'True', '--run_name', 'BIMBA_LLaVA_Qwen2_7B', '--output_dir', 'work_dirs/BIMBA_LLaVA_Qwen2_7B', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '100', '--learning_rate', '1e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '8192', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '2', '--lazy_preprocess', 'True', '--torch_compile', 'True', '--torch_compile_backend', 'inductor', '--dataloader_drop_last', 'True', '--frames_upbound', '16', '--mm_newline_position', 'grid', '--add_time_instruction', 'True', '--force_sample', 'True', '--mm_spatial_pool_stride', '2', '--report_to', 'none']\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,103] [INFO] [launch.py:256:main] process 694 spawned with command: ['/opt/conda/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=2', '--deepspeed', 'scripts/zero3.json', '--model_name_or_path', 'lmms-lab/LLaVA-Video-7B-Qwen2', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '32', '--version', 'qwen_1_5', '--data_path', '/opt/ml/code/BIMBA-LLaVA-NeXT/scripts/video/train/exp.yaml', '--image_folder', '/tmp/empty_images', '--video_folder', '/tmp', '--mm_tunable_parts=mm_vision_tower,mm_mlp_adapter,mm_language_model', '--mm_vision_tower_lr=2e-6', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--compressor_type', 'bimba', '--group_by_modality_length', 'True', '--image_aspect_ratio', 'anyres_max_9', '--image_grid_pinpoints', '(1x1),...,(6x6)', '--mm_patch_merge_type', 'spatial_unpad', '--bf16', 'True', '--run_name', 'BIMBA_LLaVA_Qwen2_7B', '--output_dir', 'work_dirs/BIMBA_LLaVA_Qwen2_7B', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '100', '--learning_rate', '1e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '8192', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '2', '--lazy_preprocess', 'True', '--torch_compile', 'True', '--torch_compile_backend', 'inductor', '--dataloader_drop_last', 'True', '--frames_upbound', '16', '--mm_newline_position', 'grid', '--add_time_instruction', 'True', '--force_sample', 'True', '--mm_spatial_pool_stride', '2', '--report_to', 'none']\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,104] [INFO] [launch.py:256:main] process 695 spawned with command: ['/opt/conda/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--deepspeed', 'scripts/zero3.json', '--model_name_or_path', 'lmms-lab/LLaVA-Video-7B-Qwen2', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '32', '--version', 'qwen_1_5', '--data_path', '/opt/ml/code/BIMBA-LLaVA-NeXT/scripts/video/train/exp.yaml', '--image_folder', '/tmp/empty_images', '--video_folder', '/tmp', '--mm_tunable_parts=mm_vision_tower,mm_mlp_adapter,mm_language_model', '--mm_vision_tower_lr=2e-6', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--compressor_type', 'bimba', '--group_by_modality_length', 'True', '--image_aspect_ratio', 'anyres_max_9', '--image_grid_pinpoints', '(1x1),...,(6x6)', '--mm_patch_merge_type', 'spatial_unpad', '--bf16', 'True', '--run_name', 'BIMBA_LLaVA_Qwen2_7B', '--output_dir', 'work_dirs/BIMBA_LLaVA_Qwen2_7B', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '100', '--learning_rate', '1e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '8192', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '2', '--lazy_preprocess', 'True', '--torch_compile', 'True', '--torch_compile_backend', 'inductor', '--dataloader_drop_last', 'True', '--frames_upbound', '16', '--mm_newline_position', 'grid', '--add_time_instruction', 'True', '--force_sample', 'True', '--mm_spatial_pool_stride', '2', '--report_to', 'none']\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,105] [INFO] [launch.py:256:main] process 696 spawned with command: ['/opt/conda/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=4', '--deepspeed', 'scripts/zero3.json', '--model_name_or_path', 'lmms-lab/LLaVA-Video-7B-Qwen2', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '32', '--version', 'qwen_1_5', '--data_path', '/opt/ml/code/BIMBA-LLaVA-NeXT/scripts/video/train/exp.yaml', '--image_folder', '/tmp/empty_images', '--video_folder', '/tmp', '--mm_tunable_parts=mm_vision_tower,mm_mlp_adapter,mm_language_model', '--mm_vision_tower_lr=2e-6', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--compressor_type', 'bimba', '--group_by_modality_length', 'True', '--image_aspect_ratio', 'anyres_max_9', '--image_grid_pinpoints', '(1x1),...,(6x6)', '--mm_patch_merge_type', 'spatial_unpad', '--bf16', 'True', '--run_name', 'BIMBA_LLaVA_Qwen2_7B', '--output_dir', 'work_dirs/BIMBA_LLaVA_Qwen2_7B', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '100', '--learning_rate', '1e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '8192', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '2', '--lazy_preprocess', 'True', '--torch_compile', 'True', '--torch_compile_backend', 'inductor', '--dataloader_drop_last', 'True', '--frames_upbound', '16', '--mm_newline_position', 'grid', '--add_time_instruction', 'True', '--force_sample', 'True', '--mm_spatial_pool_stride', '2', '--report_to', 'none']\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,105] [INFO] [launch.py:256:main] process 697 spawned with command: ['/opt/conda/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=5', '--deepspeed', 'scripts/zero3.json', '--model_name_or_path', 'lmms-lab/LLaVA-Video-7B-Qwen2', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '32', '--version', 'qwen_1_5', '--data_path', '/opt/ml/code/BIMBA-LLaVA-NeXT/scripts/video/train/exp.yaml', '--image_folder', '/tmp/empty_images', '--video_folder', '/tmp', '--mm_tunable_parts=mm_vision_tower,mm_mlp_adapter,mm_language_model', '--mm_vision_tower_lr=2e-6', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--compressor_type', 'bimba', '--group_by_modality_length', 'True', '--image_aspect_ratio', 'anyres_max_9', '--image_grid_pinpoints', '(1x1),...,(6x6)', '--mm_patch_merge_type', 'spatial_unpad', '--bf16', 'True', '--run_name', 'BIMBA_LLaVA_Qwen2_7B', '--output_dir', 'work_dirs/BIMBA_LLaVA_Qwen2_7B', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '100', '--learning_rate', '1e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '8192', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '2', '--lazy_preprocess', 'True', '--torch_compile', 'True', '--torch_compile_backend', 'inductor', '--dataloader_drop_last', 'True', '--frames_upbound', '16', '--mm_newline_position', 'grid', '--add_time_instruction', 'True', '--force_sample', 'True', '--mm_spatial_pool_stride', '2', '--report_to', 'none']\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,106] [INFO] [launch.py:256:main] process 698 spawned with command: ['/opt/conda/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=6', '--deepspeed', 'scripts/zero3.json', '--model_name_or_path', 'lmms-lab/LLaVA-Video-7B-Qwen2', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '32', '--version', 'qwen_1_5', '--data_path', '/opt/ml/code/BIMBA-LLaVA-NeXT/scripts/video/train/exp.yaml', '--image_folder', '/tmp/empty_images', '--video_folder', '/tmp', '--mm_tunable_parts=mm_vision_tower,mm_mlp_adapter,mm_language_model', '--mm_vision_tower_lr=2e-6', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--compressor_type', 'bimba', '--group_by_modality_length', 'True', '--image_aspect_ratio', 'anyres_max_9', '--image_grid_pinpoints', '(1x1),...,(6x6)', '--mm_patch_merge_type', 'spatial_unpad', '--bf16', 'True', '--run_name', 'BIMBA_LLaVA_Qwen2_7B', '--output_dir', 'work_dirs/BIMBA_LLaVA_Qwen2_7B', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '100', '--learning_rate', '1e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '8192', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '2', '--lazy_preprocess', 'True', '--torch_compile', 'True', '--torch_compile_backend', 'inductor', '--dataloader_drop_last', 'True', '--frames_upbound', '16', '--mm_newline_position', 'grid', '--add_time_instruction', 'True', '--force_sample', 'True', '--mm_spatial_pool_stride', '2', '--report_to', 'none']\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:36,107] [INFO] [launch.py:256:main] process 699 spawned with command: ['/opt/conda/bin/python3.10', '-u', 'llava/train/train_mem.py', '--local_rank=7', '--deepspeed', 'scripts/zero3.json', '--model_name_or_path', 'lmms-lab/LLaVA-Video-7B-Qwen2', '--lora_enable', 'True', '--lora_r', '128', '--lora_alpha', '32', '--version', 'qwen_1_5', '--data_path', '/opt/ml/code/BIMBA-LLaVA-NeXT/scripts/video/train/exp.yaml', '--image_folder', '/tmp/empty_images', '--video_folder', '/tmp', '--mm_tunable_parts=mm_vision_tower,mm_mlp_adapter,mm_language_model', '--mm_vision_tower_lr=2e-6', '--vision_tower', 'google/siglip-so400m-patch14-384', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--compressor_type', 'bimba', '--group_by_modality_length', 'True', '--image_aspect_ratio', 'anyres_max_9', '--image_grid_pinpoints', '(1x1),...,(6x6)', '--mm_patch_merge_type', 'spatial_unpad', '--bf16', 'True', '--run_name', 'BIMBA_LLaVA_Qwen2_7B', '--output_dir', 'work_dirs/BIMBA_LLaVA_Qwen2_7B', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '2', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '500', '--save_total_limit', '100', '--learning_rate', '1e-6', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '8192', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '2', '--lazy_preprocess', 'True', '--torch_compile', 'True', '--torch_compile_backend', 'inductor', '--dataloader_drop_last', 'True', '--frames_upbound', '16', '--mm_newline_position', 'grid', '--add_time_instruction', 'True', '--force_sample', 'True', '--mm_spatial_pool_stride', '2', '--report_to', 'none']\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:39,840] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:40,016] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:40,076] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:40,095] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:40,101] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:40,120] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:40,130] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:40,134] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m async_io: please install the libaio-dev package with apt\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\u001b[0m\n",
      "\u001b[34m#033[93m [WARNING] #033[0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/ml/code/BIMBA-LLaVA-NeXT/llava/model/llava_arch.py:238: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if slower_img_feat is not 0:\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/ml/code/BIMBA-LLaVA-NeXT/llava/model/llava_arch.py:238: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if slower_img_feat is not 0:\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/ml/code/BIMBA-LLaVA-NeXT/llava/model/llava_arch.py:238: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if slower_img_feat is not 0:\u001b[0m\n",
      "\u001b[34m/opt/ml/code/BIMBA-LLaVA-NeXT/llava/model/llava_arch.py:238: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if slower_img_feat is not 0:\u001b[0m\n",
      "\u001b[34m/opt/ml/code/BIMBA-LLaVA-NeXT/llava/model/llava_arch.py:238: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if slower_img_feat is not 0:\u001b[0m\n",
      "\u001b[34m/opt/ml/code/BIMBA-LLaVA-NeXT/llava/model/llava_arch.py:238: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if slower_img_feat is not 0:\u001b[0m\n",
      "\u001b[34m/opt/ml/code/BIMBA-LLaVA-NeXT/llava/model/llava_arch.py:238: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if slower_img_feat is not 0:\u001b[0m\n",
      "\u001b[34m/opt/ml/code/BIMBA-LLaVA-NeXT/llava/model/llava_arch.py:238: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if slower_img_feat is not 0:\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:46,489] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:46,615] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:46,835] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:46,846] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:46,885] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:46,885] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:46,904] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:46,904] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:22:46,905] [INFO] [comm.py:637:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mRank 0:  Overwriting config with {'use_pos_skipping': False, 'pos_skipping_range': 4096, 'compressor_type': 'bimba', 'temporal_pooling': 1, 'mm_spatial_pool_mode': 'bilinear'}\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:14<00:42, 14.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.33s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:13<00:40, 13.44s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:13<00:40, 13.63s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:13<00:39, 13.33s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  25%|██▌       | 1/4 [00:13<00:41, 13.99s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:28<00:28, 14.05s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:27<00:27, 13.78s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:27<00:27, 13.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:28<00:28, 14.02s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:27<00:27, 13.77s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:27<00:27, 13.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:27<00:27, 13.79s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 2/4 [00:27<00:27, 13.93s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:42<00:14, 14.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:41<00:14, 14.01s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:41<00:14, 14.01s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:42<00:14, 14.14s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:41<00:13, 14.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:41<00:14, 14.01s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:42<00:14, 14.08s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  75%|███████▌  | 3/4 [00:41<00:14, 14.05s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00,  9.74s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00, 11.22s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:45<00:00,  9.75s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:45<00:00, 11.25s/it]\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00,  9.75s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00, 11.23s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:45<00:00,  9.84s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:45<00:00, 11.40s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00,  9.74s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00, 11.22s/it]\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:45<00:00,  9.79s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:45<00:00, 11.31s/it]\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00,  9.75s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:44<00:00, 11.23s/it]\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:45<00:00,  9.85s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 4/4 [00:45<00:00, 11.40s/it]\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34mYou are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:143: FutureWarning: The `vocab_size` attribute is deprecated and will be removed in v4.42, Please use `text_config.vocab_size` instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mNCCL version 2.18.1+cuda12.1\u001b[0m\n",
      "\u001b[34mRank 0:  Loading vision tower: google/siglip-so400m-patch14-384\u001b[0m\n",
      "\u001b[34m[2025-05-11 10:23:57,264] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 1148, num_elems = 15.76B\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]#015Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.31s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.31s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.33s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.33s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.36s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.36s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.35s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.42s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.20s/it]\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.20s/it]\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.20s/it]\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.14s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSome weights of LlavaQwenForCausalLM were not initialized from the model checkpoint at lmms-lab/LLaVA-Video-7B-Qwen2 and are newly initialized: ['model.compressor.layers.0.mixer.A_b_log', 'model.compressor.layers.0.mixer.A_log', 'model.compressor.layers.0.mixer.D', 'model.compressor.layers.0.mixer.D_b', 'model.compressor.layers.0.mixer.conv1d.bias', 'model.compressor.layers.0.mixer.conv1d.weight', 'model.compressor.layers.0.mixer.conv1d_b.bias', 'model.compressor.layers.0.mixer.conv1d_b.weight', 'model.compressor.layers.0.mixer.dt_proj.bias', 'model.compressor.layers.0.mixer.dt_proj.weight', 'model.compressor.layers.0.mixer.dt_proj_b.bias', 'model.compressor.layers.0.mixer.dt_proj_b.weight', 'model.compressor.layers.0.mixer.in_proj.weight', 'model.compressor.layers.0.mixer.out_proj.weight', 'model.compressor.layers.0.mixer.x_proj.weight', 'model.compressor.layers.0.mixer.x_proj_b.weight', 'model.compressor.layers.0.norm.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mRank 0:  Prompt version: qwen_1_5\u001b[0m\n",
      "\u001b[34mRank 0:  google/siglip-so400m-patch14-384 is already loaded, `load_model` called again, skipping.\u001b[0m\n",
      "\u001b[34mRank 0:  Using mm_tunable_parts: mm_vision_tower,mm_mlp_adapter,mm_language_model\u001b[0m\n",
      "\u001b[34mRank 0:  Adding LoRA adapters...\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mRank 0:  Total parameters: ~8170.34 MB)\u001b[0m\n",
      "\u001b[34mRank 0:  Trainable parameters: ~156.97 MB)\u001b[0m\n",
      "\u001b[34mRank 0:  Loading /opt/ml/input/data/training/llava_video_dataset_clean.jsonl with all sampling strategy\u001b[0m\n",
      "\u001b[34mRank 0:  Loaded 130928 samples from /opt/ml/input/data/training/llava_video_dataset_clean.jsonl\u001b[0m\n",
      "\u001b[34mRank 0:  Loaded 130928 samples from /opt/ml/code/BIMBA-LLaVA-NeXT/scripts/video/train/exp.yaml\u001b[0m\n",
      "\u001b[34mRank 0:  Formatting inputs...Skip in lazy mode\u001b[0m\n",
      "\u001b[34mRank 0:  Setting NCCL timeout to INF to avoid running errors.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \u001b[0m\n",
      "\u001b[34mdataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \u001b[0m\n",
      "\u001b[34mdataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \u001b[0m\n",
      "\u001b[34mdataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \u001b[0m\n",
      "\u001b[34mdataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \u001b[0m\n",
      "\u001b[34mdataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \u001b[0m\n",
      "\u001b[34mdataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \u001b[0m\n",
      "\u001b[34mdataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \u001b[0m\n",
      "\u001b[34mdataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=None)\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 832928 in 416 params\u001b[0m\n",
      "\u001b[34m0%|          | 0/8183 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34mERROR: ld.so: object '/libchangehostname.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 1/8183 [00:19<45:17:40, 19.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4311, 'grad_norm': 0.19315894713746476, 'learning_rate': 4.065040650406504e-09, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 1/8183 [00:19<45:17:40, 19.93s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/8183 [00:23<23:18:40, 10.26s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3606, 'grad_norm': 0.23289268051360196, 'learning_rate': 8.130081300813008e-09, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 2/8183 [00:23<23:18:40, 10.26s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/8183 [00:26<16:18:47,  7.18s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.368, 'grad_norm': 0.19198599828314808, 'learning_rate': 1.2195121951219512e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 3/8183 [00:26<16:18:47,  7.18s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 4/8183 [00:30<12:58:03,  5.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.381, 'grad_norm': 0.22823412056197193, 'learning_rate': 1.6260162601626016e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 4/8183 [00:30<12:58:03,  5.71s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 5/8183 [00:33<11:09:21,  4.91s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.407, 'grad_norm': 0.2091521457304304, 'learning_rate': 2.032520325203252e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 5/8183 [00:33<11:09:21,  4.91s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 6/8183 [00:37<10:01:08,  4.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3552, 'grad_norm': 0.21619139478016833, 'learning_rate': 2.4390243902439023e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 6/8183 [00:37<10:01:08,  4.41s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 7/8183 [00:40<9:19:35,  4.11s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.378, 'grad_norm': 0.2154032084354267, 'learning_rate': 2.8455284552845527e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 7/8183 [00:40<9:19:35,  4.11s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 8/8183 [00:44<8:50:47,  3.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3832, 'grad_norm': 0.22381939457011227, 'learning_rate': 3.252032520325203e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 8/8183 [00:44<8:50:47,  3.90s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 9/8183 [00:47<8:31:39,  3.76s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4533, 'grad_norm': 0.19940498236760767, 'learning_rate': 3.658536585365853e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 9/8183 [00:47<8:31:39,  3.76s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 10/8183 [00:51<8:19:14,  3.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3923, 'grad_norm': 0.21927312985844455, 'learning_rate': 4.065040650406504e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 10/8183 [00:51<8:19:14,  3.67s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 11/8183 [00:54<8:24:29,  3.70s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4224, 'grad_norm': 0.20436546328096333, 'learning_rate': 4.4715447154471546e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 11/8183 [00:54<8:24:29,  3.70s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 12/8183 [00:58<8:15:05,  3.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.394, 'grad_norm': 0.22568428163862903, 'learning_rate': 4.878048780487805e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 12/8183 [00:58<8:15:05,  3.64s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 13/8183 [01:02<8:16:59,  3.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3802, 'grad_norm': 0.18058551397322478, 'learning_rate': 5.2845528455284554e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 13/8183 [01:02<8:16:59,  3.65s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 14/8183 [01:05<8:10:14,  3.60s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4071, 'grad_norm': 0.23283209173585834, 'learning_rate': 5.6910569105691055e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 14/8183 [01:05<8:10:14,  3.60s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 15/8183 [01:09<8:04:32,  3.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4102, 'grad_norm': 0.22256979475920374, 'learning_rate': 6.097560975609756e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 15/8183 [01:09<8:04:32,  3.56s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 16/8183 [01:12<8:01:13,  3.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.347, 'grad_norm': 0.19812619361430642, 'learning_rate': 6.504065040650406e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 16/8183 [01:12<8:01:13,  3.54s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 17/8183 [01:16<8:00:59,  3.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.35, 'grad_norm': 0.21351324804354163, 'learning_rate': 6.910569105691057e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 17/8183 [01:16<8:00:59,  3.53s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 18/8183 [01:19<7:57:46,  3.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4081, 'grad_norm': 0.19880810266317403, 'learning_rate': 7.317073170731706e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 18/8183 [01:19<7:57:46,  3.51s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 19/8183 [01:23<8:10:33,  3.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3953, 'grad_norm': 0.2072411241135487, 'learning_rate': 7.723577235772358e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 19/8183 [01:23<8:10:33,  3.61s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 20/8183 [01:26<8:06:18,  3.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4568, 'grad_norm': 0.20674360131249736, 'learning_rate': 8.130081300813008e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 20/8183 [01:26<8:06:18,  3.57s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 21/8183 [01:30<8:04:11,  3.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.382, 'grad_norm': 0.19675030624317594, 'learning_rate': 8.536585365853659e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 21/8183 [01:30<8:04:11,  3.56s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 22/8183 [01:33<8:00:44,  3.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3865, 'grad_norm': 0.21400676435259164, 'learning_rate': 8.943089430894309e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 22/8183 [01:33<8:00:44,  3.53s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 23/8183 [01:37<7:57:48,  3.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3562, 'grad_norm': 0.20762958137221046, 'learning_rate': 9.349593495934959e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 23/8183 [01:37<7:57:48,  3.51s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 24/8183 [01:40<7:55:47,  3.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4683, 'grad_norm': 0.21899541741040224, 'learning_rate': 9.75609756097561e-08, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 24/8183 [01:40<7:55:47,  3.50s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 25/8183 [01:44<7:54:57,  3.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3617, 'grad_norm': 0.20643628249962828, 'learning_rate': 1.016260162601626e-07, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 25/8183 [01:44<7:54:57,  3.49s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 26/8183 [01:47<7:54:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4345, 'grad_norm': 0.22770809908587383, 'learning_rate': 1.0569105691056911e-07, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 26/8183 [01:47<7:54:06,  3.49s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 27/8183 [01:51<7:52:59,  3.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3793, 'grad_norm': 0.20935465446358975, 'learning_rate': 1.097560975609756e-07, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 27/8183 [01:51<7:52:59,  3.48s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 28/8183 [01:54<7:53:02,  3.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3827, 'grad_norm': 0.221081277940033, 'learning_rate': 1.1382113821138211e-07, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 28/8183 [01:54<7:53:02,  3.48s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 29/8183 [01:58<7:56:01,  3.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3311, 'grad_norm': 0.22049839268566765, 'learning_rate': 1.1788617886178862e-07, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 29/8183 [01:58<7:56:01,  3.50s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 30/8183 [02:01<7:58:21,  3.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3457, 'grad_norm': 0.20193135585758096, 'learning_rate': 1.219512195121951e-07, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 30/8183 [02:01<7:58:21,  3.52s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 31/8183 [02:05<7:56:37,  3.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.476, 'grad_norm': 0.21254298491345433, 'learning_rate': 1.260162601626016e-07, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 31/8183 [02:05<7:56:37,  3.51s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 32/8183 [02:08<7:54:48,  3.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.3894, 'grad_norm': 0.1981949365834661, 'learning_rate': 1.3008130081300813e-07, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 32/8183 [02:08<7:54:48,  3.50s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 33/8183 [02:12<7:52:35,  3.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4232, 'grad_norm': 0.22641136351556285, 'learning_rate': 1.3414634146341465e-07, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 33/8183 [02:12<7:52:35,  3.48s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 34/8183 [02:15<7:51:48,  3.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.4253, 'grad_norm': 0.2464473311596675, 'learning_rate': 1.3821138211382114e-07, 'epoch': 0.0}\u001b[0m\n",
      "\u001b[34m0%|          | 34/8183 [02:15<7:51:48,  3.47s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import sagemaker, boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# 👇  we’re already inside the BIMBA folder, so just use cwd()\n",
    "SRC_DIR = Path.cwd()                         # /home/.../user-default-efs/BIMBA\n",
    "REQ_FILE = SRC_DIR / \"BIMBA-LLaVA-NeXT\" / \"requirements.txt\"\n",
    "IMAGE_URI = \"495467399120.dkr.ecr.us-west-2.amazonaws.com/bimba-train:latest\"\n",
    "\n",
    "estimator = PyTorch(\n",
    "    image_uri        = IMAGE_URI,\n",
    "    entry_point      = \"train_entrypoint.py\",   # file is right here\n",
    "    source_dir       = str(SRC_DIR),            # <-- fixed path\n",
    "    dependencies     = [str(REQ_FILE)],\n",
    "    role             = role,\n",
    "    instance_type    = \"ml.p4d.24xlarge\",\n",
    "    instance_count   = 1,\n",
    "    framework_version = \"2.1\",\n",
    "    py_version       = \"py310\",\n",
    "    base_job_name    = \"bimba-train\",\n",
    "    disable_profiler = True,\n",
    ")\n",
    "\n",
    "estimator.fit(\n",
    "    inputs = {\n",
    "        \"training\": \"s3://echodata25/data/llava_video_dataset_clean.jsonl\"\n",
    "    },\n",
    "    wait   = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ac81270d-e8f7-4d66-bec1-750bcb7b7f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/11/25 09:40:55] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> SageMaker Python SDK will collect telemetry to help us better  <a href=\"file:///opt/conda/lib/python3.12/site-packages/sagemaker/telemetry/telemetry_logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">telemetry_logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/conda/lib/python3.12/site-packages/sagemaker/telemetry/telemetry_logging.py#91\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">91</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         understand our user's needs, diagnose issues, and deliver      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         additional features.                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To opt out of telemetry, please disable via TelemetryOptOut    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         parameter in SDK defaults config. For more information, refer  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         to                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://sagemaker.readthedocs.io/en/stable/overview.html#confi</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">guring-and-using-defaults-with-the-sagemaker-python-sdk.</span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/11/25 09:40:55]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m SageMaker Python SDK will collect telemetry to help us better  \u001b]8;id=98565;file:///opt/conda/lib/python3.12/site-packages/sagemaker/telemetry/telemetry_logging.py\u001b\\\u001b[2mtelemetry_logging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=517303;file:///opt/conda/lib/python3.12/site-packages/sagemaker/telemetry/telemetry_logging.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         understand our user's needs, diagnose issues, and deliver      \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         additional features.                                           \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To opt out of telemetry, please disable via TelemetryOptOut    \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         parameter in SDK defaults config. For more information, refer  \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         to                                                             \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://sagemaker.readthedocs.io/en/stable/overview.html#confi\u001b[0m \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mguring-and-using-defaults-with-the-sagemaker-python-sdk.\u001b[0m       \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "# from sagemaker.estimator import Estimator\n",
    "# import sagemaker\n",
    "\n",
    "# role      = sagemaker.get_execution_role()\n",
    "# SRC_DIR   = Path.cwd()          # /home/.../BIMBA\n",
    "# IMAGE_URI = \"495467399120.dkr.ecr.us-west-2.amazonaws.com/bimba-train:latest\"\n",
    "\n",
    "# estimator = Estimator(\n",
    "#     image_uri       = IMAGE_URI,        # ← custom image\n",
    "#     role            = role,\n",
    "#     entry_point     = \"train_entrypoint.py\",\n",
    "#     source_dir      = str(SRC_DIR),\n",
    "#     instance_type   = \"ml.p4d.24xlarge\",\n",
    "#     instance_count  = 1,\n",
    "#     base_job_name   = \"bimba-train\",\n",
    "#     disable_profiler= True,\n",
    "# )\n",
    "\n",
    "# estimator.fit(\n",
    "#     inputs = {\n",
    "#         \"training\": \"s3://echodata25/data/llava_video_dataset.jsonl\"\n",
    "#     },\n",
    "#     wait=True,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee3b9f2-cf0d-4354-bc18-87829d18916b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
